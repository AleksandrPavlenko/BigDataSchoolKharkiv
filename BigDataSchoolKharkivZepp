{"paragraphs":[{"text":"%dep\nz.load(\"com.databricks:spark-csv_2.10:1.5.0\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:1.6.3\")\nz.load(\"org.apache.kafka:kafka_2.10:0.8.2.1\")","dateUpdated":"2016-11-26T17:43:19+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480162413355_-2085674541","id":"20161126-141333_884046992","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@3e54f30b\n"},"dateCreated":"2016-11-26T14:13:33+0200","dateStarted":"2016-11-26T17:43:19+0200","dateFinished":"2016-11-26T17:43:29+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"text":"import org.apache.spark.sql.types._\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.classification.LogisticRegression\n\n\nval schema = new StructType(Array(\n    StructField(\"Id\", LongType),\n    StructField(\"ProductId\", StringType),\n    StructField(\"UserId\", StringType),\n    StructField(\"ProfileName\", StringType),\n    StructField(\"HelpfulnessNumerator\", StringType),\n    StructField(\"HelpfulnessDenominator\", StringType),\n    StructField(\"Score\", DoubleType),\n    StructField(\"Time\", StringType),\n    StructField(\"Summary\", StringType),\n    StructField(\"Text\", StringType)))\n\nval df = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .schema(schema)\n    .load(\"/Users/apavlenko/Downloads/Reviews.csv\")\n    \nval training = df.select($\"Id\" as \"id\", $\"Text\" as \"text\", $\"Score\" as \"score\")\nval binarizer = new Binarizer()\n    .setInputCol(\"score\")\n    .setOutputCol(\"label\")\n    .setThreshold(4.5)\n\nval trainingBinarized = binarizer.transform(training).select($\"id\", $\"text\", $\"label\")\n\nval tokenizer = new Tokenizer()\n    .setInputCol(\"text\")\n    .setOutputCol(\"words\")\n\nval hashingTF = new HashingTF()\n    .setNumFeatures(10000)\n    .setInputCol(tokenizer.getOutputCol)\n    .setOutputCol(\"features\")\n\nval lr = new LogisticRegression()\n    .setMaxIter(10)\n    .setRegParam(0.0001)\n\nval pipeline = new Pipeline()\n    .setStages(Array(tokenizer, hashingTF, lr))\n\nval model = pipeline.fit(trainingBinarized)","dateUpdated":"2016-11-26T17:44:24+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480162428030_-902541024","id":"20161126-141348_2081899642","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.sql.types._\n\nimport org.apache.spark.ml.Pipeline\n\nimport org.apache.spark.mllib.linalg.Vector\n\nimport org.apache.spark.ml.feature._\n\nimport org.apache.spark.ml.classification.LogisticRegression\n\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,LongType,true), StructField(ProductId,StringType,true), StructField(UserId,StringType,true), StructField(ProfileName,StringType,true), StructField(HelpfulnessNumerator,StringType,true), StructField(HelpfulnessDenominator,StringType,true), StructField(Score,DoubleType,true), StructField(Time,StringType,true), StructField(Summary,StringType,true), StructField(Text,StringType,true))\n\ndf: org.apache.spark.sql.DataFrame = [Id: bigint, ProductId: string, UserId: string, ProfileName: string, HelpfulnessNumerator: string, HelpfulnessDenominator: string, Score: double, Time: string, Summary: string, Text: string]\n\ntraining: org.apache.spark.sql.DataFrame = [id: bigint, text: string, score: double]\n\nbinarizer: org.apache.spark.ml.feature.Binarizer = binarizer_2f21e47fdea6\n\ntrainingBinarized: org.apache.spark.sql.DataFrame = [id: bigint, text: string, label: double]\n\ntokenizer: org.apache.spark.ml.feature.Tokenizer = tok_181c7d67eec6\n\nhashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_dca81138851c\n\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_cdc935fb3799\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_6a3c20125477\n\nmodel: org.apache.spark.ml.PipelineModel = pipeline_6a3c20125477\n"},"dateCreated":"2016-11-26T14:13:48+0200","dateStarted":"2016-11-26T17:44:24+0200","dateFinished":"2016-11-26T17:45:23+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport kafka.serializer.StringDecoder\nimport org.apache.spark.mllib.linalg.VectorUDT\n\ncase class Item(text: String, probability: Double, prediction: Boolean)\n\nval ssc = new StreamingContext(sc, Seconds(10))\n    val kafkaParams = Map[String, String](\n      \"metadata.broker.list\" -> \"localhost:9092\"\n    )\n\nval kafkaStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, Set(\"test\"))\nkafkaStream foreachRDD { rdd =>\n      val data = rdd.values.zipWithIndex()\n      val df = data.toDF(\"text\",\"id\")\n\n      val result = model.transform(df).map(row => {\n        val probability = row.getAs[VectorUDT](\"probability\").toString\n        val probArray = probability.substring(1,probability.length-1).split(\",\")\n        val probRes = java.lang.Double.max(probArray(0).toDouble,probArray(1).toDouble)\n        val pred = row.getAs[VectorUDT](\"prediction\").toString.toDouble == 1.0\n        Item(row.getAs(\"text\"), probRes, pred)\n      })\n      \n      if (!result.isEmpty()) result.toDF().registerTempTable(\"data\")\n    }\nssc.start()","dateUpdated":"2016-11-26T17:48:06+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480162444062_1716090638","id":"20161126-141404_1673603951","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.streaming.kafka.KafkaUtils\n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nimport kafka.serializer.StringDecoder\n\nimport org.apache.spark.mllib.linalg.VectorUDT\n\ndefined class Item\n\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@166ecf8\n\nkafkaParams: scala.collection.immutable.Map[String,String] = Map(metadata.broker.list -> localhost:9092)\n\nkafkaStream: org.apache.spark.streaming.dstream.InputDStream[(String, String)] = org.apache.spark.streaming.kafka.DirectKafkaInputDStream@501b13cb\n"},"dateCreated":"2016-11-26T14:14:04+0200","dateStarted":"2016-11-26T17:48:06+0200","dateFinished":"2016-11-26T17:48:08+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"%sql\n\nselect probability as Probability, prediction as IS_Positive, text as Text from data","dateUpdated":"2016-11-26T17:53:47+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Probability","index":0,"aggr":"sum"}],"values":[{"name":"IS_Positive","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"Probability","index":0,"aggr":"sum"},"yAxis":{"name":"IS_Positive","index":1,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480162624862_1405839650","id":"20161126-141704_2024565310","result":{"code":"SUCCESS","type":"TABLE","msg":"Probability\tIS_Positive\tText\n0.6400576255309294\tfalse\tIt was awful, I want to return ticket #BigDataSchoolKharkiv\"\n","comment":"","msgTable":[[{"key":"IS_Positive","value":"0.6400576255309294"},{"key":"IS_Positive","value":"false"},{"key":"IS_Positive","value":"It was awful, I want to return ticket #BigDataSchoolKharkiv\""}]],"columnNames":[{"name":"Probability","index":0,"aggr":"sum"},{"name":"IS_Positive","index":1,"aggr":"sum"},{"name":"Text","index":2,"aggr":"sum"}],"rows":[["0.6400576255309294","false","It was awful, I want to return ticket #BigDataSchoolKharkiv\""]]},"dateCreated":"2016-11-26T14:17:04+0200","dateStarted":"2016-11-26T17:53:47+0200","dateFinished":"2016-11-26T17:53:47+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"","dateUpdated":"2016-11-26T14:52:37+0200","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1480164747799_2030890215","id":"20161126-145227_561706808","dateCreated":"2016-11-26T14:52:27+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:165"}],"name":"Big Data School","id":"2C2X6AMR6","angularObjects":{"2C3AVCHCV:shared_process":[],"2C27JH294:shared_process":[],"2C55QQ14R:shared_process":[],"2C2H4DAH5:shared_process":[],"2C1QANAXR:shared_process":[],"2C3DDCTU6:shared_process":[],"2C4RPX3QY:shared_process":[],"2C3UFSGT9:shared_process":[],"2C394KA9X:shared_process":[],"2C3YHNZQV:shared_process":[],"2C2YCU5N8:shared_process":[],"2C3ZVN7H6:shared_process":[],"2C1VGR1ES:shared_process":[],"2C22SED44:shared_process":[],"2C33GMKP2:shared_process":[],"2C1VGVF1Z:shared_process":[],"2C24XMRT4:shared_process":[],"2C1MEH38C:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}